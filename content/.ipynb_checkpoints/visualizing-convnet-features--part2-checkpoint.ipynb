{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Title: Visualizing features, receptive fields, and classes in neural networks from \"scratch\" with Tensorflow 2. Part 2: Visualizing features and receptive fields\n",
    "- Date: 2020-05-19 09:01\n",
    "- Category: Neural networks\n",
    "- Tags: VGG16, tensorflow, neural networks, convolutional neural networks, receptive fields\n",
    "- Slug: tensorflow-2-feature-visualization-visualizing-features\n",
    "- Authors: Tim Sainburg\n",
    "- githuburl: https://github.com/timsainb/tensorflow-2-feature-visualization-notebooks\n",
    "- Summary: A few examples of feature visualization in convolutional neural networks with Tensorflow 2.0. In this part, we look at visualizing classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "In the [last section](http://timsainburg.com/tensorflow-2-feature-visualization-visualizing-classes) I talked about visualizing neural network classes by synthesizing images to maximally activate class labels. This time, we're going to do the same thing with convolutional filters rather than class labels. In addition, we'll play with a few other ways to look at the receptive fields of convolutional filters. All the figured here are generated from a single [self-contained Jupyter notebook](https://github.com/timsainb/tensorflow-2-feature-visualization-notebooks).\n",
    "\n",
    "\n",
    "There are a few sections to this series:\n",
    "1. **[Visualizing classes](http://timsainburg.com/tensorflow-2-feature-visualization-visualizing-classes)** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/timsainb/tensorflow-2-feature-visualization-notebooks/blob/master/0.0-Visualizing-classes.ipynb)\n",
    "2. **[Visualizing features](http://timsainburg.com/tensorflow-2-feature-visualization-visualizing-features)** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/timsainb/tensorflow-2-feature-visualization-notebooks/blob/master/0.1-Visualize-receptive-fields-and-features.ipynb)\n",
    "3. **[Reconstructing images from features](http://timsainburg.com/tensorflow-2-feature-visualization-reconstructing-images)** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/timsainb/tensorflow-2-feature-visualization-notebooks/blob/master/0.2-Reconstructing-images-from-layer-activation.ipynb)\n",
    "4. **[Transferring features between images](http://timsainburg.com/tensorflow-2-feature-visualization-deepdream-style-transfer)** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/timsainb/tensorflow-2-feature-visualization-notebooks/blob/master/0.3-Visualize-deepdream-style-transfer.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthesizing images that maximally activate convolutional filters / artificial neurons\n",
    "We can use the same technique as above to synthesize images that maximally activate any neuron in the neural network. This will give us some idea about what that neuron is doing. Like the human visual stream, early neurons are responsive to simple textures and colors, while later neurons respond most strongly to complex patterns. As you move further into the network, the patterns have spatial structure that spans a larger range of the image. Below are a sample of a few images synthesized to maximally activate neurons at each layer of VGG 16. Note that for convolutional layers, these activations are maximized for each convolutional filter across the entire image, not just individual artifical neurons (we'll take a look at individual receptive fields in below). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "<p align=\"center\">\n",
    "  <img style=\"float:center\",  src=\"assets/feature_viz/features_all_rotate.jpg\", width=\"80%\" max_width=\"800px\">\n",
    "    </p>\n",
    "</div>\n",
    "<div style=\"text-align:center; clear:both\">\n",
    "<p style=\"text-align:center;font-style:italic\"><span style=\"font-style: normal; font-weight:bold\">Image synthesized to maximally activate filters from lower to higher layers in VGG16.</p> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthesizing images that maximally activate receptive fields of individual neurons\n",
    "The previous example shows the maximum activation for a convolutional filter across a whole image. We could also pinpoint a single neuron (filter) and find an image that maximally activates that neuron. Because single neurons in CNNs are only receptive to a small region of the image (the receptive field), the synthesized image should only end up changing that region of the image. Here, we'll maximally activate single neurons to take a look at their receptive field. Deeper into the network receptive fields get larger, and more complex. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "<p align=\"center\">\n",
    "  <img style=\"float:center\",  src=\"assets/feature_viz/all_receptive_fields_flipped.jpg\", width=\"80%\" max_width=\"800px\">\n",
    "    </p>\n",
    "</div>\n",
    "<div style=\"text-align:center; clear:both\">\n",
    "<p style=\"text-align:center;font-style:italic\"><span style=\"font-style: normal; font-weight:bold\">Synthesized image to maximally activate a single artificial neuron with a receptive field in the center of the image.</p> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:40:19.045087Z",
     "start_time": "2020-05-20T20:40:19.037423Z"
    }
   },
   "source": [
    "It might also be useful to take a look at those receptive fields, zoomed in. \n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "<p align=\"center\">\n",
    "  <img style=\"float:center\",  src=\"assets/feature_viz/all_receptive_fields_zoom_rotate.jpg\", width=\"80%\" max_width=\"800px\">\n",
    "    </p>\n",
    "</div>\n",
    "<div style=\"text-align:center; clear:both\">\n",
    "<p style=\"text-align:center;font-style:italic\"><span style=\"font-style: normal; font-weight:bold\">Synthesized image to maximally activate a single artificial neuron with a receptive field in the center of the image. Zoomed into recepetive field area.</p> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing activation maps from neurons in different layers of a network\n",
    "Another common way of visualizing features of a neural network is to take a look at what neurons are activated across an image for each filter type in a layer. Early in the network, there will be neurons that are specifically interested in colors or edges. Later, images respond to more complex features, sparsely present throughout the image. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T22:08:32.438373Z",
     "start_time": "2020-05-20T22:08:32.431643Z"
    }
   },
   "source": [
    "<div style=\"text-align:center\">\n",
    "<p align=\"center\">\n",
    "  <img style=\"float:center\",  src=\"assets/feature_viz/all_filter_activations.jpg\", width=\"80%\" max_width=\"800px\">\n",
    "    </p>\n",
    "</div>\n",
    "<div style=\"text-align:center; clear:both\">\n",
    "<p style=\"text-align:center;font-style:italic\"><span style=\"font-style: normal; font-weight:bold\">Activation maps for convolutional filters in response to an image of <a href=\"assets/feature_viz/all_filter_activations.jpg\">Astronaut Eileen Collins.</a></p> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the code needed to reproduce these images and videos are available in [self contained Jupyter notebooks](https://github.com/timsainb/tensorflow-2-feature-visualization-notebooks). \n",
    "\n",
    "In the [next section](http://timsainburg.com/tensorflow-2-feature-visualization-reconstructing-images), we'll try to see whether enough information is contained in the CNN filters to reconstruct the image at different layers. Thanks for reading."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
