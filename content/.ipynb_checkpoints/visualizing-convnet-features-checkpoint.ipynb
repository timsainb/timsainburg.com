{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Title: Visualizing features, receptive fields, and classes in neural networks from \"scratch\" with Tensorflow 2. Part 1: Visualizing classes\n",
    "- Date: 2020-05-19 08:01\n",
    "- Category: Neural networks\n",
    "- Tags: VGG16, tensorflow, neural networks, convolutional neural networks, receptive fields\n",
    "- Slug: tensorflow-2-feature-visualization-visualizing-classes\n",
    "- Authors: Tim Sainburg\n",
    "- githuburl: https://github.com/timsainb/tensorflow-2-feature-visualization-notebooks\n",
    "- Summary: A few examples of feature visualization in convolutional neural networks with Tensorflow 2.0. In this part, we look at visualizing classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "I spent some time playing around with  visualizing neural network features and artificial neuron receptive fields using Tensorflow 2.X. I wrote this code because I wanted to visualize the neural network features for a project I'm currently working on, but the best library out there ([Lucid](https://github.com/tensorflow/lucid)) is written for Tensorflow 1.X. In my implementation, I [provide a self-contained Jupyter notebook](tensorflow-2-feature-visualization-notebooks) that visualizes features for the VGG16 convolutional neural networks (these methods would translate to any network). I didn't read very much into how other people have done feature visualizations, so there is likely some overlap, but some of the methods I use might be novel. \n",
    "\n",
    "\n",
    "There are a few sections to this series:\n",
    "1. **[Visualizing classes](http://timsainburg.com/tensorflow-2-feature-visualization-visualizing-classes)** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/timsainb/tensorflow-2-feature-visualization-notebooks/blob/master/0.0-Visualizing-classes.ipynb)\n",
    "2. **[Visualizing features](http://timsainburg.com/tensorflow-2-feature-visualization-visualizing-features)** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/timsainb/tensorflow-2-feature-visualization-notebooks/blob/master/0.1-Visualize-receptive-fields-and-features.ipynb)\n",
    "3. **[Reconstructing images from features](http://timsainburg.com/tensorflow-2-feature-visualization-reconstructing-images)** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/timsainb/tensorflow-2-feature-visualization-notebooks/blob/master/0.2-Reconstructing-images-from-layer-activation.ipynb)\n",
    "4. **[Transferring features between images](http://timsainburg.com/tensorflow-2-feature-visualization-deepdream-style-transfer)** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/timsainb/tensorflow-2-feature-visualization-notebooks/blob/master/0.3-Visualize-deepdream-style-transfer.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Network outline\n",
    "The network we're going to use here is VGG16, a classic neural network architecture that performs very well on the ImageNet dataset. VGG16 is very simple, it's just a few convolutional layers mixed with max pooling layers, and finally a few fully connected layers. \n",
    "\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "<p align=\"center\">\n",
    "  <img style=\"float:center\",  src=\"assets/feature_viz/vgg16.png\", width=\"470px\" max_width=\"470px\">\n",
    "</div>\n",
    "</p>\n",
    "<div style=\"text-align:center; clear:both\">\n",
    "<p style=\"text-align:center;font-style:italic\"><span style=\"font-style: normal; font-weight:bold\">VGG network outline. (<a href=\"https://www.cs.toronto.edu/~frossard/post/vgg16/\">image source</a>)</p> \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthesizing images that maximally activate class labels\n",
    "I'm going to start with visualizing class labels because I think that these visualizations are the most visually intuitive: they are somewhat reminiscent of things you will see in real life. The point of this method is to start with an image of random noise, then iteratively manipulate that noise until it maximally activates a class label in an artificial neural network. In other words, we are shifting the pixels around in an image until the neural network thinks it looks like the thing it was trained to perceive (for example, a dog, cat, or firetruck). The network is trained to know what these images look like, so it expects to see certain features in that image. By manipulating the image to be percieved as that class (e.g. dog) the image will begin to possess features of that image (e.g. ears, paws). \n",
    "\n",
    "The main technique here is just generating a random, and then performing gradient ascent on that image until your neuron of interest in the network is maximally activated. There are some tricks to that though. The main one (in my experience) is manipulating the the frequency of the features the network is interested in. If you train directly from noise, the network will mostly care about high-frequencies, and the resulting image won't have very much global coherency. To deal with this, I ended up doing two things: (1) gradually zooming in over each iteration of training. and (2) blurring the image between each epoch of training. As a result, the generated images look a little airbrushed, but you can see some more global structure, for example entire heads or bodies of birds. \n",
    "\n",
    "Below, you can see some synthesized images for a few ImageNet classes from the VGG16 network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "<p align=\"center\">\n",
    "  <img style=\"float:center\",  src=\"assets/feature_viz/VGG_classes.png\", width=\"80%\" max_width=\"800px\">\n",
    "</div>\n",
    "</p>\n",
    "<div style=\"text-align:center; clear:both\">\n",
    "<p style=\"text-align:center;font-style:italic\"><span style=\"font-style: normal; font-weight:bold\">Classes visualized from a pretrained VGG network</p> \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The synthesized images end up looking like textures. By changing the amount of bluring and zooming you do, you can manipulate the level of detail of the textures (this also changes the high-frequency vs airbrush look of the synthesized images). \n",
    "\n",
    "We can also visualize those classes during each step of gradient ascent. \n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "<p align=\"center\">\n",
    "<video width=\"640\" height=\"480\" style=\"text-align:center; display:block; margin:auto\" controls>\n",
    "  <source src=\"assets/feature_viz/classes.mp4\" type=\"video/mp4\">\n",
    "Your browser does not support the video tag.\n",
    "</video>\n",
    "    </p>\n",
    "</div>\n",
    "<div style=\"text-align:center; clear:both\">\n",
    "<p style=\"text-align:center;font-style:italic\"><span style=\"font-style: normal; font-weight:bold\">VGG classes for each step of synthesis</p> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for fun, here's one of a few classes of birds:\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "<p align=\"center\">\n",
    "  <img style=\"float:center\",  src=\"assets/feature_viz/birds.png\", width=\"80%\" max_width=\"800px\">\n",
    "    </p>\n",
    "</div>\n",
    "<div style=\"text-align:center; clear:both\">\n",
    "<p style=\"text-align:center;font-style:italic\"><span style=\"font-style: normal; font-weight:bold\">Bird textures synthesized from VGG16 classes.</p> \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the code needed to reproduce these images and videos are available in [self contained Jupyter notebooks](https://github.com/timsainb/tensorflow-2-feature-visualization-notebooks). \n",
    "\n",
    "In the [next section](http://timsainburg.com/tensorflow-2-feature-visualization-visualizing-features), we'll try something similar to visualize filters and receptive fields throughout different layers of the network. Thanks for reading."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
