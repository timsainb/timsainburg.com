{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Title: Visualizing features, receptive fields, and classes in neural networks from \"scratch\" with Tensorflow 2. Part 4: DeepDream and style transfer\n",
    "- Date: 2020-05-19 11:01\n",
    "- Category: Neural networks\n",
    "- Tags: VGG16, tensorflow, neural networks, convolutional neural networks, receptive fields\n",
    "- Slug: tensorflow-2-feature-visualization-deepdream-style-transfer\n",
    "- Authors: Tim Sainburg\n",
    "- githuburl: https://github.com/timsainb/tensorflow-2-feature-visualization-notebooks\n",
    "- Summary: A few examples of feature visualization in convolutional neural networks with Tensorflow 2.0. In this part, we look at filtering images using different layers of a deep neural network. We also perform style transfer, by modifying an image to have similar layer activations as a second source image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "In the [last section](http://timsainburg.com/tensorflow-2-feature-visualization-reconstructing-images) I talked about reconstructing images based upon their layer activations at different levels of a deep neural network. This time, we're going to try to modify images by transferring styles between different layers of the network. All the figured here are generated from a single [self-contained Jupyter notebook](https://github.com/timsainb/tensorflow-2-feature-visualization-notebooks).\n",
    "\n",
    "\n",
    "There are a few sections to this series:\n",
    "1. **[Visualizing classes](http://timsainburg.com/tensorflow-2-feature-visualization-visualizing-classes)** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/timsainb/tensorflow-2-feature-visualization-notebooks/blob/master/0.0-Visualizing-classes.ipynb)\n",
    "2. **[Visualizing features](http://timsainburg.com/tensorflow-2-feature-visualization-visualizing-features)** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/timsainb/tensorflow-2-feature-visualization-notebooks/blob/master/0.1-Visualize-receptive-fields-and-features.ipynb)\n",
    "3. **[Reconstructing images from features](http://timsainburg.com/tensorflow-2-feature-visualization-reconstructing-images)** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/timsainb/tensorflow-2-feature-visualization-notebooks/blob/master/0.2-Reconstructing-images-from-layer-activation.ipynb)\n",
    "4. **[Transferring features between images](http://timsainburg.com/tensorflow-2-feature-visualization-deepdream-style-transfer)** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/timsainb/tensorflow-2-feature-visualization-notebooks/blob/master/0.3-Visualize-deepdream-style-transfer.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deepdream, modifying images with neural network filters and classes\n",
    "In the second section, we synthesized images by generating an image of noise, and modifying that image until it maximally activated a neuron/filter in a deep neural network. Here, we want to use those filters to modify images. To do that, instead of starting with noise, we can try starting from an image. The midpoint between the starting and the generated image can have features that look styled by the CNN's filters: \n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "<p align=\"center\">\n",
    "  <img style=\"float:center\",  src=\"assets/feature_viz/deepdream-filters-all.jpg\", width=\"80%\" max_width=\"800px\">\n",
    "    </p>\n",
    "</div>\n",
    "<div style=\"text-align:center; clear:both\">\n",
    "<p style=\"text-align:center;font-style:italic\"><span style=\"font-style: normal; font-weight:bold\">Stylizations of an image of <a href=\"assets/feature_viz/all_filter_activations.jpg\">Astronaut Eileen Collins</a> by modifying the activations of neurons at different levels of the VGG16 CNN architecture.</p> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same thing with feature classes, by activating the final layer of the network:\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "<p align=\"center\">\n",
    "  <img style=\"float:center\",  src=\"assets/feature_viz/deepdream_features.png\", width=\"80%\" max_width=\"800px\">\n",
    "    </p>\n",
    "</div>\n",
    "<div style=\"text-align:center; clear:both\">\n",
    "<p style=\"text-align:center;font-style:italic\"><span style=\"font-style: normal; font-weight:bold\">Stylizations of an image of <a href=\"assets/feature_viz/all_filter_activations.jpg\">Astronaut Eileen Collins</a> by modifying the activations of neurons at the class layer of the VGG16 CNN architecture.</p> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize those classes during each step of gradient ascent. \n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "<p align=\"center\">\n",
    "<video width=\"640\" height=\"480\"  style=\"text-align:center; display:block; margin:auto\" controls>\n",
    "  <source src=\"assets/feature_viz/deepdream.mp4\" type=\"video/mp4\">\n",
    "Your browser does not support the video tag.\n",
    "</video>\n",
    "    </p>\n",
    "</div>\n",
    "<div style=\"text-align:center; clear:both\">\n",
    "<p style=\"text-align:center;font-style:italic\"><span style=\"font-style: normal; font-weight:bold\">Manipulation of an image  of <a href=\"assets/feature_viz/all_filter_activations.jpg\">Astronaut Eileen Collins</a>, to class activations in the final layer of VGG16.</p> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Style transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also manipulate the style of the image to match a second image, called style transfer. There are many papers working on this. The approach here is simple, and could be extended in many ways (even just performing style transfer over multiple layers simultaneously). \n",
    "\n",
    "Here, we transfer the activation profile of an image of <a href=\"assets/feature_viz/all_filter_activations.jpg\">Astronaut Eileen Collins</a> to an image of the painting <a href=\"assets/feature_viz/all_filter_activations.jpg\">The Starry Night</a> by Van Gogh. \n",
    "\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "<p align=\"center\">\n",
    "  <img style=\"float:center\",  src=\"assets/feature_viz/starry_night-style-transfer.png\", width=\"80%\" max_width=\"800px\">\n",
    "    </p>\n",
    "</div>\n",
    "<div style=\"text-align:center; clear:both\">\n",
    "<p style=\"text-align:center;font-style:italic\"><span style=\"font-style: normal; font-weight:bold\">Stylizations of an image of <a href=\"assets/feature_viz/all_filter_activations.jpg\">Astronaut Eileen Collins</a> by modifying the activations of neurons at the class layer of the VGG16 CNN architecture.</p> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the code needed to reproduce these images and videos are available in [self contained Jupyter notebooks](https://github.com/timsainb/tensorflow-2-feature-visualization-notebooks). \n",
    "\n",
    "That's it! This is the final section. Thanks for reading. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
